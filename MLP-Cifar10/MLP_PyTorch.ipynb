{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCP1ukCcVlkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFDALkjJVlkt",
        "colab_type": "code",
        "outputId": "2e2e7587-498e-4fc9-cc3a-2780bd7e6ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Carregar os datasets\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170024960/170498071 [00:17<00:00, 6530107.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITp-eTbrVlky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(dataset=dataset_train, shuffle=True)\n",
        "test_loader = DataLoader(dataset=dataset_test, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcT2mxmhVlk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definir a arquitetura MLP\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32*32, 100)\n",
        "        self.fc2 = nn.Linear(100, 50)\n",
        "        self.fc3= nn.Linear(50, 10)\n",
        "        #self.activation_function = nn.Sigmoid()\n",
        "        self.activation_function = nn.ReLU()\n",
        "        self.soft_max = nn.Softmax(dim=1)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32*32)\n",
        "        x = self.activation_function(self.fc1(x))\n",
        "        x = self.activation_function(self.fc2(x))\n",
        "        x = self.activation_function(self.fc3(x))\n",
        "        x = self.soft_max(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO8MYoH7Vlk4",
        "colab_type": "code",
        "outputId": "516d5f68-d5a4-4e5e-d49f-666702e0cc07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "model = MLP()\n",
        "print(model)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=1024, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (activation_function): ReLU()\n",
            "  (soft_max): Softmax()\n",
            ")\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOsVwiAZVlk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definir otimizador e loss\n",
        "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
        "loss_fn = torch.nn.MSELoss().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOoJmZj-VllG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Avaliar o modelo aqui (no conjunto de teste)\n",
        "def validate():\n",
        "  \n",
        "    model.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      \n",
        "        for data in test_loader:\n",
        "          \n",
        "            images, labels = data\n",
        "            \n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "    return correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS3wvzv-VllD",
        "colab_type": "code",
        "outputId": "5ad455b0-1bde-44e8-974a-54497e257160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1794
        }
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Realizar o treinamento aqui\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  avg_loss = 0\n",
        "  hits = 0\n",
        "  total = 0\n",
        "  #progress_bar = tqdm(train_loader)\n",
        "  \n",
        "  for i, (X,y) in enumerate(train_loader):\n",
        "      model.train()\n",
        "      #progress_bar.set_description(\"Epoch \" + str(epoch))\n",
        "      \n",
        "      # Send to device\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      # convert label to one-hot tensor\n",
        "      onehot_y = np.zeros(10)\n",
        "      onehot_y[y.item()] = 1\n",
        "      onehot_y = torch.cuda.FloatTensor(onehot_y)    \n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # get output of model\n",
        "      output = model(X)\n",
        "      \n",
        "      \n",
        "      # calculate loss and backpropagates\n",
        "      loss = loss_fn(output, onehot_y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      avg_loss += loss.item()\n",
        "      \n",
        "      _, pred = torch.max(output.data,1)\n",
        "      hits += (pred == y).sum().item()\n",
        "      total += y.size(0)\n",
        "      train_accuracy = hits / total\n",
        "      \n",
        "      #progress_bar.set_postfix(xentropy='%.3f' % (avg_loss / (i + 1)),\n",
        "      #                 acc='%.3f' % (train_accuracy))\n",
        "\n",
        "#      if i % 10000 == 9999:\n",
        "#        print(\"[\", epoch, \",\", i+1,\"] \" \"loss: \", avg_loss / 10000)\n",
        "#        avg_loss = 0\n",
        "\n",
        "  test_accuracy = validate()\n",
        "  \n",
        "  #tqdm.write('test_acc: %.3f' % (test_accuracy))\n",
        "  print(\"[\", epoch, \"]\", \"test_acc: \", (test_accuracy), \"train_acc:\", (train_accuracy), \"loss:\", avg_loss / len(train_loader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:29, 6530107.23it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ 0 ] test_acc:  0.2253 train_acc: 0.21354 loss: 0.08705977026007924\n",
            "[ 1 ] test_acc:  0.3176 train_acc: 0.29244 loss: 0.08245192813069989\n",
            "[ 2 ] test_acc:  0.3345 train_acc: 0.32396 loss: 0.07990337094082026\n",
            "[ 3 ] test_acc:  0.3439 train_acc: 0.33992 loss: 0.07860952974329652\n",
            "[ 4 ] test_acc:  0.3535 train_acc: 0.35164 loss: 0.07763421745451388\n",
            "[ 5 ] test_acc:  0.3589 train_acc: 0.3603 loss: 0.07692509121911138\n",
            "[ 6 ] test_acc:  0.3568 train_acc: 0.36832 loss: 0.07615795637980656\n",
            "[ 7 ] test_acc:  0.3755 train_acc: 0.37604 loss: 0.0755592403987661\n",
            "[ 8 ] test_acc:  0.3651 train_acc: 0.38222 loss: 0.07502730949485885\n",
            "[ 9 ] test_acc:  0.3648 train_acc: 0.38672 loss: 0.07449655624136574\n",
            "[ 10 ] test_acc:  0.3736 train_acc: 0.39532 loss: 0.07398176760308943\n",
            "[ 11 ] test_acc:  0.3856 train_acc: 0.39738 loss: 0.07367780449467844\n",
            "[ 12 ] test_acc:  0.3728 train_acc: 0.40082 loss: 0.07319603488063563\n",
            "[ 13 ] test_acc:  0.3643 train_acc: 0.4066 loss: 0.07282234460966923\n",
            "[ 14 ] test_acc:  0.3886 train_acc: 0.41078 loss: 0.07248108767396069\n",
            "[ 15 ] test_acc:  0.3777 train_acc: 0.41566 loss: 0.0721848215065124\n",
            "[ 16 ] test_acc:  0.3773 train_acc: 0.41874 loss: 0.07183917770375851\n",
            "[ 17 ] test_acc:  0.3763 train_acc: 0.42146 loss: 0.07156319237567754\n",
            "[ 18 ] test_acc:  0.3839 train_acc: 0.4227 loss: 0.0712986360789716\n",
            "[ 19 ] test_acc:  0.389 train_acc: 0.4276 loss: 0.07099455796471596\n",
            "[ 20 ] test_acc:  0.3919 train_acc: 0.42566 loss: 0.07093666767992803\n",
            "[ 21 ] test_acc:  0.3734 train_acc: 0.4319 loss: 0.07057754941686618\n",
            "[ 22 ] test_acc:  0.392 train_acc: 0.43384 loss: 0.07036233559110494\n",
            "[ 23 ] test_acc:  0.3991 train_acc: 0.43438 loss: 0.07016328341757165\n",
            "[ 24 ] test_acc:  0.3986 train_acc: 0.44046 loss: 0.06979976905646146\n",
            "[ 25 ] test_acc:  0.3943 train_acc: 0.44086 loss: 0.06984610139175175\n",
            "[ 26 ] test_acc:  0.3887 train_acc: 0.441 loss: 0.06977650108186195\n",
            "[ 27 ] test_acc:  0.3623 train_acc: 0.4436 loss: 0.06948302759043054\n",
            "[ 28 ] test_acc:  0.3921 train_acc: 0.44844 loss: 0.06913449109111942\n",
            "[ 29 ] test_acc:  0.397 train_acc: 0.44984 loss: 0.0690094133503853\n",
            "[ 30 ] test_acc:  0.3973 train_acc: 0.44766 loss: 0.068884192403607\n",
            "[ 31 ] test_acc:  0.4021 train_acc: 0.45124 loss: 0.06864086163804184\n",
            "[ 32 ] test_acc:  0.3871 train_acc: 0.4521 loss: 0.06862805030183945\n",
            "[ 33 ] test_acc:  0.378 train_acc: 0.45674 loss: 0.06824087633426491\n",
            "[ 34 ] test_acc:  0.3995 train_acc: 0.45558 loss: 0.06837368874963266\n",
            "[ 35 ] test_acc:  0.399 train_acc: 0.45772 loss: 0.06807256464167975\n",
            "[ 36 ] test_acc:  0.3959 train_acc: 0.45882 loss: 0.06785915704827987\n",
            "[ 37 ] test_acc:  0.3912 train_acc: 0.461 loss: 0.06769494669269797\n",
            "[ 38 ] test_acc:  0.3974 train_acc: 0.46244 loss: 0.0675947253611399\n",
            "[ 39 ] test_acc:  0.3999 train_acc: 0.46376 loss: 0.06762852637996145\n",
            "[ 40 ] test_acc:  0.394 train_acc: 0.46752 loss: 0.06728367059870939\n",
            "[ 41 ] test_acc:  0.3896 train_acc: 0.46728 loss: 0.06724960388004372\n",
            "[ 42 ] test_acc:  0.3968 train_acc: 0.466 loss: 0.06714943380062667\n",
            "[ 43 ] test_acc:  0.3959 train_acc: 0.47018 loss: 0.06692915837697573\n",
            "[ 44 ] test_acc:  0.3868 train_acc: 0.46922 loss: 0.0667777164894394\n",
            "[ 45 ] test_acc:  0.3976 train_acc: 0.4713 loss: 0.06673793491501444\n",
            "[ 46 ] test_acc:  0.4019 train_acc: 0.4742 loss: 0.06665434848460332\n",
            "[ 47 ] test_acc:  0.3976 train_acc: 0.47344 loss: 0.06655098429987458\n",
            "[ 48 ] test_acc:  0.4032 train_acc: 0.47418 loss: 0.06639482316700086\n",
            "[ 49 ] test_acc:  0.4001 train_acc: 0.47522 loss: 0.06637794119491643\n",
            "[ 50 ] test_acc:  0.3946 train_acc: 0.47954 loss: 0.06602535507708622\n",
            "[ 51 ] test_acc:  0.3807 train_acc: 0.4801 loss: 0.06593479873743678\n",
            "[ 52 ] test_acc:  0.401 train_acc: 0.48008 loss: 0.06600918050601795\n",
            "[ 53 ] test_acc:  0.3963 train_acc: 0.48236 loss: 0.06577488958657944\n",
            "[ 54 ] test_acc:  0.4059 train_acc: 0.48184 loss: 0.06572334618958242\n",
            "[ 55 ] test_acc:  0.3996 train_acc: 0.48482 loss: 0.06546311646206736\n",
            "[ 56 ] test_acc:  0.3867 train_acc: 0.48652 loss: 0.06547139582276394\n",
            "[ 57 ] test_acc:  0.398 train_acc: 0.48496 loss: 0.06537203736608647\n",
            "[ 58 ] test_acc:  0.3942 train_acc: 0.4871 loss: 0.06523655130315337\n",
            "[ 59 ] test_acc:  0.4012 train_acc: 0.48616 loss: 0.06521466751220296\n",
            "[ 60 ] test_acc:  0.3909 train_acc: 0.48958 loss: 0.06493165351269\n",
            "[ 61 ] test_acc:  0.4008 train_acc: 0.49092 loss: 0.06490400397987735\n",
            "[ 62 ] test_acc:  0.3996 train_acc: 0.49032 loss: 0.06477629807691312\n",
            "[ 63 ] test_acc:  0.3929 train_acc: 0.49194 loss: 0.06474659818842249\n",
            "[ 64 ] test_acc:  0.4086 train_acc: 0.49294 loss: 0.06463171421628688\n",
            "[ 65 ] test_acc:  0.3909 train_acc: 0.49426 loss: 0.06453963453476083\n",
            "[ 66 ] test_acc:  0.3832 train_acc: 0.49294 loss: 0.06448147069719938\n",
            "[ 67 ] test_acc:  0.403 train_acc: 0.49692 loss: 0.06437189225599635\n",
            "[ 68 ] test_acc:  0.4095 train_acc: 0.49892 loss: 0.06425949128760221\n",
            "[ 69 ] test_acc:  0.4022 train_acc: 0.49778 loss: 0.06419058963761502\n",
            "[ 70 ] test_acc:  0.3972 train_acc: 0.49946 loss: 0.064163176022476\n",
            "[ 71 ] test_acc:  0.4052 train_acc: 0.50152 loss: 0.06388706614344808\n",
            "[ 72 ] test_acc:  0.398 train_acc: 0.49776 loss: 0.06389864106318306\n",
            "[ 73 ] test_acc:  0.3941 train_acc: 0.50012 loss: 0.0638202228784888\n",
            "[ 74 ] test_acc:  0.4043 train_acc: 0.50188 loss: 0.06365076251053856\n",
            "[ 75 ] test_acc:  0.3823 train_acc: 0.50398 loss: 0.06350035017113734\n",
            "[ 76 ] test_acc:  0.4056 train_acc: 0.50174 loss: 0.06353788738669779\n",
            "[ 77 ] test_acc:  0.3946 train_acc: 0.50766 loss: 0.06305562727422397\n",
            "[ 78 ] test_acc:  0.4094 train_acc: 0.50516 loss: 0.0632503688885929\n",
            "[ 79 ] test_acc:  0.3949 train_acc: 0.50704 loss: 0.06333386962219899\n",
            "[ 80 ] test_acc:  0.3996 train_acc: 0.50784 loss: 0.06304966538476806\n",
            "[ 81 ] test_acc:  0.3971 train_acc: 0.50764 loss: 0.06299723976979177\n",
            "[ 82 ] test_acc:  0.3856 train_acc: 0.50578 loss: 0.06331694633863547\n",
            "[ 83 ] test_acc:  0.4064 train_acc: 0.50938 loss: 0.06295978141082434\n",
            "[ 84 ] test_acc:  0.4055 train_acc: 0.51064 loss: 0.06277649588062371\n",
            "[ 85 ] test_acc:  0.3976 train_acc: 0.51168 loss: 0.06252263623300916\n",
            "[ 86 ] test_acc:  0.4032 train_acc: 0.51128 loss: 0.06273347221836689\n",
            "[ 87 ] test_acc:  0.4022 train_acc: 0.51398 loss: 0.0627133937199809\n",
            "[ 88 ] test_acc:  0.3994 train_acc: 0.51644 loss: 0.062481443319374615\n",
            "[ 89 ] test_acc:  0.3913 train_acc: 0.51368 loss: 0.06259242143562549\n",
            "[ 90 ] test_acc:  0.4067 train_acc: 0.51602 loss: 0.06227859535300475\n",
            "[ 91 ] test_acc:  0.3954 train_acc: 0.51514 loss: 0.06230563642905192\n",
            "[ 92 ] test_acc:  0.4051 train_acc: 0.5147 loss: 0.062190847635586285\n",
            "[ 93 ] test_acc:  0.3899 train_acc: 0.51802 loss: 0.061996697194383214\n",
            "[ 94 ] test_acc:  0.4118 train_acc: 0.52076 loss: 0.06184616380367943\n",
            "[ 95 ] test_acc:  0.4001 train_acc: 0.51838 loss: 0.0618178772051659\n",
            "[ 96 ] test_acc:  0.3971 train_acc: 0.51928 loss: 0.06185978511769043\n",
            "[ 97 ] test_acc:  0.3999 train_acc: 0.5218 loss: 0.06171373726167611\n",
            "[ 98 ] test_acc:  0.3813 train_acc: 0.51992 loss: 0.061821562226903125\n",
            "[ 99 ] test_acc:  0.3955 train_acc: 0.5215 loss: 0.061697748984897535\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}